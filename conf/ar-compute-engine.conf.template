[default]

# mongo server ip location
mongo_host=localhost

# mongo server port 
mongo_port=27017

# core database used by argo
mongo_core_db = argo_core

# mongo authentication
# mongo_user = 
# mongo_pass = 

# declare the mode of ARGOeu
# can be: local or cluster
mode=cluster

# declare the serialization framework
# can be: avro or none
serialization=none

# declare if prefilter data must be cleaned after upload to hdfs
prefilter_clean=true
sync_clean=true

# Provide maximum number of recomputations that can run in parallel.
recomp_threshold=1

[logging]

# mode for logging (syslog,file,none)
log_mode=syslog

# log level status
log_level=DEBUG

# If log_mode equals file - uncomment to set log file path:
# log_file=/var/log/ar-compute/ar-compute.log

# Hadoop clients log level and log appender
# If you want to log via SYSLOG make sure
# an appropriate appender is defined in hadoop
# log4j.properties file and just add the name
# of this appender in the following line. I.e. 
# if you define a new appender named SYSLOG 
# change console to SYSLOG, or just add 
# SYSLOG appender in the following line
hadoop_log_root=INFO,console

[connectors]

sync_exec=/usr/libexec/ar-sync/
sync_path=/var/lib/ar-sync/

[jobs]

# Here are declared available tenants and available jobs
# for each tenant (tenant/job names are case-sensitive)
# The order of declarations is as follows:
#
# tenants=TenantA,TenantB
# TenantA_jobs=Job1,Job2,Job3
# TenantB_jobs=Job4,Job5
# TenantA_prefilter=prefilter_exec (optional)
#
# Declare available tenants
tenants=TenantA

# For a declared tenant declare it's jobs by using 
# {Tenant_Name}_jobs conformance
TenantA_jobs = Job_1

# For a given tenant provide prefiltering wrapper (if needed)
TenantA_prefilter=prefilter-tenantA.py

[sampling]

s_period=1440
s_interval=5
